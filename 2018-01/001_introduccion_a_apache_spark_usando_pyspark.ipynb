{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FIUBA Organización de Datos 75.06**\n",
    "# **Introduccion a [Apache Spark](https://spark.apache.org/) usando [pySpark]((https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura\n",
    "\n",
    "En Spark, para poder realizar una tarea la comunicacion ocurre entre un **driver** y una serie de **executors** (ejecutores). El driver tiene **jobs** y para ejecutarlos se rompen en **tareas (tasks)** que se envian a los executors. Los resultados de esas tareas se envian denuevo al **driver**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este IPython Notebook la funcion del driver de Spark se ejecuta dentro del kernel asociado al notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al utilizar localmente o conectandose a un cluster, el software driver \n",
    "es **[PySpark shell](https://spark.apache.org/docs/latest/programming-guide.html#using-the-shell)**. Este contiene el main loop del programa que **creara datasets distribuidos (RDDs)** en el cluster y **aplicará operaciones (transformations & actions)** a esos datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de Cluster\n",
    "\n",
    "En este diagrama podemos ver un ejemplo de un cluster, suponiendo que estamos corriendo una aplicacion o un determinada tarea en el cluster, se pueden ver en el grafico, marcados en violeta una serie de cores dedicados a ese job o aplicacion.\n",
    "\n",
    "![executors](http://spark-mooc.github.io/web-assets/images/executors.png)\n",
    "\n",
    "A alto nivel una aplicacion de Spark Consiste en un programa **driver** que **lanza** varias **operaciones paralelas en multiples executors JVMs que corren en un cluster o localmente en la misma maquina** (localmente puede correr cada uno en cada core disponible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext\n",
    "\n",
    "Al utilizar Spark, uno crea una nueva aplicacion de Spark creando un `SparkContext`, el cual nos permitirá interactuar con el API de Spark. \n",
    "\n",
    "Cuando se crea el `SparkContext` se le pide al master algunos cores para poder ejecutar tasks. El master separar estos cores para esa aplicacion, no siendo utilizados por otras aplicaciones.\n",
    "\n",
    "Al usar el entorno de lab desde el IPython Notebook el `SparkContext` esta creado en la variable `sc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "try: \n",
    "    type(sc)\n",
    "except NameError:\n",
    "    sc = pyspark.SparkContext('local[*]')    \n",
    "    \n",
    "type(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mas información sobre como inicializar el entorno de spark en un contexto de cluster ver [https://spark.apache.org/docs/latest/programming-guide.html#initializing-spark](https://spark.apache.org/docs/latest/programming-guide.html#initializing-spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construyendo RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los **RRDs (Resilent Distributed Datasets)**, son colecciones tolerantes a fallos cuyos elementos pueden ser operados en paralelo. Se pueden crear de a partir de colecciones o **a partir de external datasets (distribuidos o no)** que veremos con algunos ejemplos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paralelizando una coleccion de python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente creamos una coleccion de datos en python que usaremos para crear el RDD usando `sc.parallelize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creamos 1000 enteros en una lista\n",
    "integersList = range(1,1001)\n",
    "#integersList\n",
    "len(integersList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paralelizamos la coleccion utilizando 8 particiones o slices\n",
    "## Esta operacion es una transformacion de datos en un RDD\n",
    "## Dado que Spark usa lazy evaluation, no corren jobs de Spark\n",
    "## hasta el momento\n",
    "integersListRDD = sc.parallelize(integersList, 6)\n",
    "type(integersListRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## podemos ver tambien otra informacion interesante del RDD\n",
    "## el numero de particiones\n",
    "integersListRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## el conjunto de transformaciones que se aplica\n",
    "print(integersListRDD.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## para ver mas metodos disponibles del RDD\n",
    "help(integersListRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué sucede en el cluster?: Información distribuida\n",
    "\n",
    "En Spark los datasets son representados como una lista de entradas, la cual se rompe en distintas particiones, cada una guarda en un maquina distinta del cluster. \n",
    "\n",
    "Cada particion tiene un unico subconjunto de las entradas de la lista. La abstraccion que usa Spark para manipularlos se conoce como \"Resilent Distributed Datasets\" (RDDs) y una de las particularidades que tienen es que estan disponibles en memoria (de ser posible).\n",
    "\n",
    "El escenario de nuestro ejemplo se veria de la siguiente forma si consideramos algunos executors\n",
    "\n",
    "![partitions](http://spark-mooc.github.io/web-assets/images/partitions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creando RDDs desde external Datasets\n",
    "\n",
    "PySpark puede crear datasets distribuidos desde cualquier storage soportado por Hadoop (local filesystem, HDFS, Cassandra, HBase, AWS S3, etc). Soportando archivos de texto y otros tipos de formatos de entrada soportados por Hadoop ([SequenceFiles](http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html) y [InputFormat](http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/InputFormat.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargando un archivo de texto\n",
    "\n",
    "Aprovechando que tenemos: [Complete Works of William Shakespeare](http://www.gutenberg.org/ebooks/100) from [Project Gutenberg](http://www.gutenberg.org/wiki/Main_Page). Vamos a convertir el archivo de texto en un RDD usando el método `SparkContext.textFile()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creamos el RDD a partir de un archivo de texto\n",
    "shakespeareRDD = sc.textFile('data/shakespeare.txt',8)\n",
    "## aplicamos una accion para tomar los 15 items del RDD\n",
    "shakespeareRDD.take(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones: `map()`\n",
    "\n",
    "Hasta el momento **creamos datasets distribuidos que se han dividido en varias particiones, donde cada particion se encuentra en una maquina de nuestro cluster**, veamos que sucede al hacer una operacion a nuestro dataset.\n",
    "\n",
    "Vamos a partir de la operacion mas usual que podemos llegar a querer hacer sobre nuestro **'por cada uno de los elementos del dataset hagamos una accion y apliquemos devolvamos un resultado'**. Esto se logra con una **transformacion `map()`** que aplicara la función f a cada resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integersListRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## aplicamos una transformacion map para restar a todos 1\n",
    "## aplicamos la accion take para mostrar 10 resultados\n",
    "# ver generacion de tuplas y uso de funciones de python en pyspark.\n",
    "integersListRDD.map(lambda x: x*2).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## aplicamos una transformacion map para restar a todos 1\n",
    "## aplicamos la accion take para mostrar 10 resultados\n",
    "\n",
    "integersListRDD.map(lambda x: (x**2, x, x-1)).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generamos una tupla que tenga el valor original, el anterior y siguiente\n",
    "## aplicamos una transformacion map para restar a todos 1\n",
    "## aplicamos la accion take para mostrar 10 resultados\n",
    "\n",
    "integersListRDD.map(lambda a: (a, a-1, a+1)).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué sucede en el cluster?\n",
    "\n",
    "Cuando ejecutamos `map()` en el dataset, un unico **stage** de tareas se lanza. Un **stage** es un grupo de tareas que van a realizar el mismo computo pero con distintos datos de entrada. Una tarea entonces se lanzara para cada una de las particiones.\n",
    "\n",
    "En el grafico de abajo vemos que sucede entonces con cada una de las particiones al ejecutarse la tarea y el resultado que producen.\n",
    "\n",
    "![foo](http://spark-mooc.github.io/web-assets/images/map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acciones: `collect()`, `count()`\n",
    "\n",
    "Luego de haber aplicado una transformacion en el RDD, podemos \n",
    "queres volver a obtener informacion en el **driver**, podemos utilizar una accion `collect()`, usualmente se utiliza despues de alguna operacion que limite la cantidad de resultados (`filter()`) para asegurarnos que el resultado devuelto entre en la memoria disponible del driver.\n",
    "\n",
    "Ya usamos anteriormente otra accion `take()` para traer un numero de resultados al driver.\n",
    "\n",
    "Volviendo a nuestro ejemplo con los numero enteros podemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## aplicamos una transformacion map para restar a todos 1\n",
    "## aplicamos la accion take para mostrar 10 resultados\n",
    "\n",
    "subRDD = integersListRDD.map(lambda a: a-1)\n",
    "print(subRDD.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el cluster sucede la siguiente situacion, al realizarse el `collect()`\n",
    "devolviendo los resultados a SparkContext.\n",
    "\n",
    "![collect](http://spark-mooc.github.io/web-assets/images/collect.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra accion interesante es `count()` que permite contar la cantidad de elementos en el RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# por ejemplo para los RDD con los que estuvimos trabajando\n",
    "\n",
    "print(integersListRDD.count())\n",
    "print(subRDD.count())\n",
    "print(shakespeareRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mas Transformaciones: `filter()`\n",
    "\n",
    "La transformacion `filter()` transformacion permite aplicar una funcion al evaluarse solamente emitira a la salida aquellos que la funcion filtro haya devuelto `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## obtener los numeros pares\n",
    "print(integersListRDD.filter(lambda a: a % 2 == 0).take(10))\n",
    "print(integersListRDD.filter(lambda a: a % 2 == 0).count())\n",
    "print(integersListRDD.filter(lambda a: a == 2).take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminar lineas vacias de the complete works of shakespeare\n",
    "#shakespeareRDD.take(15)\n",
    "shakespeareRDD.filter(lambda a: a != \"\").take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shakespeareRDD.filter(lambda a: a != \"\").count())\n",
    "print(shakespeareRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce\n",
    "\n",
    "La accion `reduce()` reduce los elementos de un RDD a un unico valor al aplicar una funcion que toma dos parametros y retorna un unico valor.\n",
    "\n",
    "La funcion planteada tiene que ser **conmutativa y asociativa**, ya que `reduce()` **es aplicado a nivel de particion y luego nuevamente para agregar resultados de particiones**. Si esto no se respeta, los resultados de `reduce()` seran inconsistentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## la suma es asociativa y conmutativa\n",
    "print(shakespeareRDD.reduce(lambda a, b: a + b))\n",
    "print(shakespeareRDD.repartition(4).reduce(lambda a, b: a + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## la resta no no asociativa y conmutativa\n",
    "print(integersListRDD.reduce(lambda a, b: a - b))\n",
    "print(integersListRDD.repartition(4).reduce(lambda a, b: a - b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otras Acciones: `first()`,`takeOrdered()`,`top()`\n",
    "\n",
    "Estas son otras acciones que pueden ser utiles para procesar informacion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first() devuelve el primer elemento del RDD\n",
    "print(shakespeareRDD.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## traemos los 3 elementos mas pequeños\n",
    "print(integersListRDD.takeOrdered(3))\n",
    "## traemos los 3 elementos mas grandes\n",
    "print(integersListRDD.top(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se puede utilizar una funcion para fijar el orden en takeOrdered\n",
    "## por ejemplo para revertirlo\n",
    "integersListRDD.takeOrdered(3, lambda a: -a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otras acciones avanzadas para investigar son [`takeSample()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.takeSample) y [`countByValue()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.countByValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones Adicionales\n",
    "\n",
    "### `flatMap()`\n",
    "\n",
    "Similar a map, pero permite que cada item de entrada se mapee a cero o mas elementos de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleRDD = sc.parallelize([2, 3, 4])\n",
    "print(simpleRDD.map(lambda x: [1, x]).collect())\n",
    "print(simpleRDD.flatMap(lambda x: [1, x]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listadelistas = [[1,2],[2,3],[5]]\n",
    "rdd = sc.parallelize(listadelistas)\n",
    "print(rdd.flatMap(lambda x: x).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones para pair RDDs\n",
    "\n",
    "Las siguientes transformaciones aplican a RDDs donde los elementos \n",
    "son tuplas del tipo `(key, value)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### `reduceByKey()`\n",
    "\n",
    "la transformacion `reduceByKey()` junta pares que tienen la misma key y aplica la funcion a los dos valores asociados a la vez. Esta opera primero aplicando la funcion a cada particion para las mismas claves y luego a traves de las particiones (similar al reduce).\n",
    "\n",
    "### `groupByKey()`\n",
    "\n",
    "Agrupa todos los elementos de una misma clave generando una clave con una lista con los elementos.\n",
    "\n",
    "Ambas se pueden usar para resolver el mismo problema, pero reduceBykey es mas eficiente en datasets distribuidos grandes, dado que en este escenario Spark sabe que puede combinar output de un mismo key (en la misma maquina) antes de redistribuir datos entre los distintos nodos (shuffling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "pairRDD = sc.parallelize([('a', 1), ('a', 2), ('b', 1)])\n",
    "\n",
    "# usamos mapValues para mejorar el formato de impresion\n",
    "#print(pairRDD.reduceByKey(lambda a,b: a+b).collect())\n",
    "\n",
    "# Diferentes formas de sumar por clave\n",
    "# problemas de notacion python3\n",
    "#print(pairRDD.groupByKey().map(lambda (k, v): (k, sum(v)))).collect())\n",
    "\n",
    "# Using mapValues, which is recommended when they key doesn't change\n",
    "#print(pairRDD.groupByKey().mapValues(lambda x: sum(x)).collect())\n",
    "\n",
    "# reduceByKey is more efficient / scalable\n",
    "#print(pairRDD.reduceByKey(add).collect())\n",
    "\n",
    "#print(shakespeareRDD.flatMap(lambda a: a.split()))\\\n",
    "#.map(lambda a: (len(a), 1))\\\n",
    "#.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otras transformaciones interesantes para investigar son [combineByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.combineByKey) y [foldByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.foldByKey)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio: Buscando la linea de máxima longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generando tuplas para obtener la linea \n",
    "# de maxima longitud de todo el texto\n",
    "\n",
    "result = shakespeareRDD.reduce(lambda a,b: a if len(a) > len(b) else b)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
