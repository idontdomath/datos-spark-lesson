{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "try: \n",
    "    type(sc)\n",
    "except NameError:\n",
    "    sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introducción\n",
    "\n",
    "Spark SQL es un modulo del Framework de Apache Spark que permite procesar datos estructurados. A diferencia de la api de RDD, las interfaces provistas por Spark SQL brinda mas informacion sobre la estructura de datos y los computos que se van a realizar sobre esas estructuras.\n",
    "\n",
    "A partir de esto Spark SQL usa esta informacion extra para realizar optimizaciones extra, a partir del **Catalyst Optimizer**. Existen distintos tipos de API incluyendo SQL y Dataset API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Para realizar los procesamientos el mismo motor de ejecucion es utilizado, independientemente del API/languaje que estes utilizando para expresar tu computation, eso significa que es relativamente facil pasar de una implementacion a otra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SQL\n",
    "\n",
    "Una de las posibilidades es la de ejecutar SQL queries sobre distintos tipo de fuentes (desde motores relacionales a traves de JDBC/ODBC, Instalaciones de Apache, Hive hasta fuentes de datos construidas como Dataframes/Datasets desde multiples formatos (json, parquet, orc, csv, tsv, etc)). \n",
    "\n",
    "Los resultados de la ejecucion de una query es un DataSet/DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DataSets y DataFrames\n",
    "\n",
    "Un Dataset es una collecion distribuida de datos. Dataset es una interfaz que brinda los beneficios de los RDDs (tipado fuerte y la habilidad de utilizar potentes funciones lambda) con los beneficios de utilizar el optimizado engine de Spark SQL. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### DataSet\n",
    "\n",
    "Un Dataset puede ser construido desde objetos de la JVM y luego manipulado por transformaciones funcionales (map, flatMap, filter, etc) estando solo disponible su API en Scala y Java. Para mas informacion de como realizar su construccion ver el siguiente link: https://spark.apache.org/docs/latest/sql-programming-guide.html#creating-datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### DataFrame\n",
    "\n",
    "Un DataFrame es un Dataset organizado en filas (Dataset of Rows) con nombre.\n",
    "Podemos pensar en el conceptualmente como equivalente a una tabla relacional en una base de datos relacional o un data frame en R o Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Estos pueden construirse de una variedad diversa de fuentes:\n",
    "\n",
    "- Archivos de informacion estructurada (csv, tsv, json, parquet, orc).\n",
    "- Tablas en Hive\n",
    "- Bases de Datos Relacionales Externas\n",
    "- RDDs existentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# el punto de inicio de la funcionalidad es la clase SparkSession\n",
    "# podemos inicializar usando el siguiente builder\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creando DataFrames a partir de CSV\n",
    "\n",
    "Para comenzar a trabajar en algunos ejemplos usaremos el siguiente set de datos:\n",
    "\n",
    "https://www.kaggle.com/usdot/flight-delays\n",
    "\n",
    "El set de datos contiene informacion sobre retrasos de vuelos y cancelaciones durante el año 2015.\n",
    "\n",
    "En los siguientes ejemplos veremos distintas variables de como cargar estos ```DataFrames``` indicando inferencia de tipos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# carga de CSV con load indicando el formato\n",
    "df_f = spark.read.load(\"../data/flight-delays/flights.csv\",\n",
    "                    format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# variante con metodo CSV\n",
    "df_a = spark.read.csv(\"../data/flight-delays/airlines.csv\",\n",
    "                    sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "# existen otros metodos especificos para distintos formatos\n",
    "# por ejemplo: json, csv, tsv indicando el separador, parquet, orc, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[pyspark.sql.types.Row]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_f.rdd.map(lambda x: type(x)).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IATA_CODE', 'AIRLINE']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, IATA_CODE: string, AIRLINE: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IATA_CODE: string (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_a.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YEAR',\n",
       " 'MONTH',\n",
       " 'DAY',\n",
       " 'DAY_OF_WEEK',\n",
       " 'AIRLINE',\n",
       " 'FLIGHT_NUMBER',\n",
       " 'TAIL_NUMBER',\n",
       " 'ORIGIN_AIRPORT',\n",
       " 'DESTINATION_AIRPORT',\n",
       " 'SCHEDULED_DEPARTURE',\n",
       " 'DEPARTURE_TIME',\n",
       " 'DEPARTURE_DELAY',\n",
       " 'TAXI_OUT',\n",
       " 'WHEELS_OFF',\n",
       " 'SCHEDULED_TIME',\n",
       " 'ELAPSED_TIME',\n",
       " 'AIR_TIME',\n",
       " 'DISTANCE',\n",
       " 'WHEELS_ON',\n",
       " 'TAXI_IN',\n",
       " 'SCHEDULED_ARRIVAL',\n",
       " 'ARRIVAL_TIME',\n",
       " 'ARRIVAL_DELAY',\n",
       " 'DIVERTED',\n",
       " 'CANCELLED',\n",
       " 'CANCELLATION_REASON',\n",
       " 'AIR_SYSTEM_DELAY',\n",
       " 'SECURITY_DELAY',\n",
       " 'AIRLINE_DELAY',\n",
       " 'LATE_AIRCRAFT_DELAY',\n",
       " 'WEATHER_DELAY']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_f.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: integer (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: integer (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: integer (nullable = true)\n",
      " |-- SECURITY_DELAY: integer (nullable = true)\n",
      " |-- AIRLINE_DELAY: integer (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n",
      " |-- WEATHER_DELAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_f.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejecutando Queries de SQL de forma Programática\n",
    "\n",
    "La funcion ```sql``` en ```SparkSession``` le permite a nuestras aplicaciones correr queries de SQL de forma programatica y obtener el resultado como un ```DataFrame```.\n",
    "\n",
    "Para poder referenciar desde queries de SQL es necesario crear una vista temporal asociada al ```DataFrame``` conocidad como **Temporary View** para poder referenciarla en este scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# registramos el DataFrame airlines como una vista SQL temporal\n",
    "df_a.createOrReplaceTempView(\"airlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# registramos el DataFrame flights como una vista SQL temporal\n",
    "df_f.createOrReplaceTempView(\"flights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Algo a tener en cuenta es que estas vistas temporales que estamos registrando solamente estaran disponibles en esa sesion y desaparecera si la sesion a la que pertenece termina. Si queremos generar vistas globales a todas las sesiones hasta la finalizacion del programa de Spark debemos registrarlas como una **Global Temporary View** con el metodo ```createGlobalTempView```\n",
    "\n",
    "Una vez hecho esto podemos ejecutar una query usando el metodo ```sql``` de ```SparkSession```\n",
    "\n",
    "Por ejemplo podemos realizar un ejemplo simple en el cual realizamos una proyeccion de los nombres de aerolineas de la vista ```airlines```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df_airline_names = spark.sql('SELECT AIRLINE from airlines');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_airline_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             AIRLINE|\n",
      "+--------------------+\n",
      "|United Air Lines ...|\n",
      "|American Airlines...|\n",
      "|     US Airways Inc.|\n",
      "|Frontier Airlines...|\n",
      "|     JetBlue Airways|\n",
      "|Skywest Airlines ...|\n",
      "|Alaska Airlines Inc.|\n",
      "|    Spirit Air Lines|\n",
      "|Southwest Airline...|\n",
      "|Delta Air Lines Inc.|\n",
      "|Atlantic Southeas...|\n",
      "|Hawaiian Airlines...|\n",
      "|American Eagle Ai...|\n",
      "|      Virgin America|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airline_names.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df_take_5 = spark.sql('SELECT YEAR,MONTH,DAY from flights LIMIT 5');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+\n",
      "|YEAR|MONTH|DAY|\n",
      "+----+-----+---+\n",
      "|2015|    1|  1|\n",
      "|2015|    1|  1|\n",
      "|2015|    1|  1|\n",
      "|2015|    1|  1|\n",
      "|2015|    1|  1|\n",
      "+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_take_5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entendiendo la motivacion de SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Al consultar documentacion actualmente podemos preguntarnos por que el API de SparkSQL esta siendo promovida como el API default para desarrollar aplicaciones.\n",
    "\n",
    "Podemos partir entendiendo que desarrollar utilizando RDDs plantea:\n",
    "\n",
    "- **Paradigma imperativo, es decir que tenemos que indicar cada paso que queremos realizar para obtener los datos que queremos.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La principal problematica de esto es que el API de **RDDs es un API de Bajo Nivel** y esto implica:\n",
    "\n",
    "- Tenemos que indicar cada paso que queremos realizar para obtener los datos que queremos, **con optimizaciones**.\n",
    "- Estas **optimizaciones** tenemos que **considerarlas tambien hasta en los casos simples, una y otra vez.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Spark no sabe nada sobre nuestros datos** (por ejemplo que datos vamos a usar solamente, o sus tipos) de tal forma que **para realizar optimizaciones lo mejor que podemos hacer es que nuestras funciones (lambdas) apliquen esquemas a nuestros datos**.\n",
    "- **Spark no sabe nada sobre los calculos que vamos a realizar**, por lo cual **la optimizacion dependera de la implementacion de nuestros lambdas**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La idea de SparkSQL es justamente resolver esto utilizando un paradigma popular y declarativo como es SQL. \n",
    "\n",
    "Al ser **declarativo**, nosotros no realizamos especificaciones paso a paso para obtener el resultado sino que para lograrlo **vamos aplicando limitaciones a la estructura y los datos hasta obtener el resultado deseado**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Por otro lado al tener un **conjunto de operaciones especificas y acotadas definidas en SQL** y donde **es necesario indicar la estructura de los datos que tenemos y que queremos obtener en un resultado**, esto **nos ayuda a poder tener versiones optimizadas de esas operaciones en el framework**. Este Trabajo es realizado por Catalyst Optimizer. Para aquellos interesados en un deep dive mas profundo les recomendamos el siguiente Articulo, mas alla de la breve introduccion que realizaremos: https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Nota: A su vez reduce el overhead de serializacion de pyspark, dado que todas las operaciones estan implementadas en Scala, python solo provee wrapper de alto nivel.\n",
    "\n",
    "Antes de avanzar entendiendo como funciona el optimizador de queries en SparkSQL, vamos a hacer una breve introduccion a SQL, en particular a los statements de su lenguaje de consultas que vamos a utilizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Muy Breve Introduccion a SQL\n",
    "\n",
    "SQL (Structured Query Language) es un lenguaje universal de programacion diseñado para manejar informacion en bases de datos. Al tener una sintaxis standard popular por su simplicidad, es soportado por todos los motores de bases de datos relacionales asi como en otro tipo de motores, como SparkSQL.\n",
    "\n",
    "Usualmente con SQL podemos:\n",
    "\n",
    "- Definir Estructuras de Datos (DDL)\n",
    "- Manipular datos (DML)\n",
    "- Consultar Datos (DQL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Esencialmente nos vamos a centrar en entender la sintaxis para realizar consultas que es la usualmente utilizada en motores de Big Data como SparkSQL.\n",
    "\n",
    "Para ello entenderemos los siguientes statements:\n",
    "\n",
    "- **SELECT (Proyeccion para obtener datos de una tabla especifica)**\n",
    "    - Todas las columnas\n",
    "    - Especificar columnas por nombre y indicarles alias.\n",
    "    - ```DISTINCT``` (Valores unicos para una o mas columnas)\n",
    "    - Uso de funciones predefinidas: ```COUNT()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **JOIN (Nos permite combinar multiples tablas para cruzar informacion)**\n",
    "    - ```ON``` (para indicar la condicion de join).\n",
    "    - Tipos: ```INNER JOIN, LEFT JOIN, RIGHT JOIN, OUTER JOIN```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Filtrado y Ordenamiento**\n",
    "    - WHERE\n",
    "        - Operadores de Comparacion: ```=``` (igual a) , ```<>``` or ```!=``` (no igual a), y otros que se explican solos: ```>, <, >=, <=```.\n",
    "        - Operadores Logicos: ```AND, OR, NOT, IN, BETWEEN, LIKE```.\n",
    "    - ```ORDER BY (ASC, DESC)```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Agregacion**\n",
    "    - ```GROUP BY``` (Separacion de datos en grupos)\n",
    "        - Uso de funciones predefinidas: ```COUNT()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|             AIRLINE|total_cancelled|\n",
      "+--------------------+---------------+\n",
      "|Southwest Airline...|          16043|\n",
      "|Atlantic Southeas...|          15231|\n",
      "|American Eagle Ai...|          15025|\n",
      "|American Airlines...|          10919|\n",
      "|Skywest Airlines ...|           9960|\n",
      "|United Air Lines ...|           6573|\n",
      "|     JetBlue Airways|           4276|\n",
      "|     US Airways Inc.|           4067|\n",
      "|Delta Air Lines Inc.|           3824|\n",
      "|    Spirit Air Lines|           2004|\n",
      "|Alaska Airlines Inc.|            669|\n",
      "|Frontier Airlines...|            588|\n",
      "|      Virgin America|            534|\n",
      "|Hawaiian Airlines...|            171|\n",
      "+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airline_names.columns\n",
    "query = 'SELECT airlines.AIRLINE, COUNT(flights.FLIGHT_NUMBER) as total_cancelled\\\n",
    "                        FROM flights INNER JOIN airlines\\\n",
    "                        ON flights.AIRLINE = airlines.IATA_CODE\\\n",
    "                        WHERE CANCELLED = 1\\\n",
    "                        GROUP BY airlines.AIRLINE\\\n",
    "                        ORDER BY total_cancelled DESC'\n",
    "\n",
    "df_sql = spark.sql(query);\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['total_cancelled DESC NULLS LAST], true\n",
      "+- 'Aggregate ['airlines.AIRLINE], ['airlines.AIRLINE, 'COUNT('flights.FLIGHT_NUMBER) AS total_cancelled#285]\n",
      "   +- 'Filter ('CANCELLED = 1)\n",
      "      +- 'Join Inner, ('flights.AIRLINE = 'airlines.IATA_CODE)\n",
      "         :- 'UnresolvedRelation `flights`\n",
      "         +- 'UnresolvedRelation `airlines`\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "AIRLINE: string, total_cancelled: bigint\n",
      "Sort [total_cancelled#285L DESC NULLS LAST], true\n",
      "+- Aggregate [AIRLINE#83], [AIRLINE#83, count(FLIGHT_NUMBER#15) AS total_cancelled#285L]\n",
      "   +- Filter (CANCELLED#34 = 1)\n",
      "      +- Join Inner, (AIRLINE#14 = IATA_CODE#82)\n",
      "         :- SubqueryAlias `flights`\n",
      "         :  +- Relation[YEAR#10,MONTH#11,DAY#12,DAY_OF_WEEK#13,AIRLINE#14,FLIGHT_NUMBER#15,TAIL_NUMBER#16,ORIGIN_AIRPORT#17,DESTINATION_AIRPORT#18,SCHEDULED_DEPARTURE#19,DEPARTURE_TIME#20,DEPARTURE_DELAY#21,TAXI_OUT#22,WHEELS_OFF#23,SCHEDULED_TIME#24,ELAPSED_TIME#25,AIR_TIME#26,DISTANCE#27,WHEELS_ON#28,TAXI_IN#29,SCHEDULED_ARRIVAL#30,ARRIVAL_TIME#31,ARRIVAL_DELAY#32,DIVERTED#33,... 7 more fields] csv\n",
      "         +- SubqueryAlias `airlines`\n",
      "            +- Relation[IATA_CODE#82,AIRLINE#83] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [total_cancelled#285L DESC NULLS LAST], true\n",
      "+- Aggregate [AIRLINE#83], [AIRLINE#83, count(FLIGHT_NUMBER#15) AS total_cancelled#285L]\n",
      "   +- Project [FLIGHT_NUMBER#15, AIRLINE#83]\n",
      "      +- Join Inner, (AIRLINE#14 = IATA_CODE#82)\n",
      "         :- Project [AIRLINE#14, FLIGHT_NUMBER#15]\n",
      "         :  +- Filter ((isnotnull(CANCELLED#34) && (CANCELLED#34 = 1)) && isnotnull(AIRLINE#14))\n",
      "         :     +- Relation[YEAR#10,MONTH#11,DAY#12,DAY_OF_WEEK#13,AIRLINE#14,FLIGHT_NUMBER#15,TAIL_NUMBER#16,ORIGIN_AIRPORT#17,DESTINATION_AIRPORT#18,SCHEDULED_DEPARTURE#19,DEPARTURE_TIME#20,DEPARTURE_DELAY#21,TAXI_OUT#22,WHEELS_OFF#23,SCHEDULED_TIME#24,ELAPSED_TIME#25,AIR_TIME#26,DISTANCE#27,WHEELS_ON#28,TAXI_IN#29,SCHEDULED_ARRIVAL#30,ARRIVAL_TIME#31,ARRIVAL_DELAY#32,DIVERTED#33,... 7 more fields] csv\n",
      "         +- Filter isnotnull(IATA_CODE#82)\n",
      "            +- Relation[IATA_CODE#82,AIRLINE#83] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(4) Sort [total_cancelled#285L DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(total_cancelled#285L DESC NULLS LAST, 200)\n",
      "   +- *(3) HashAggregate(keys=[AIRLINE#83], functions=[count(FLIGHT_NUMBER#15)], output=[AIRLINE#83, total_cancelled#285L])\n",
      "      +- Exchange hashpartitioning(AIRLINE#83, 200)\n",
      "         +- *(2) HashAggregate(keys=[AIRLINE#83], functions=[partial_count(FLIGHT_NUMBER#15)], output=[AIRLINE#83, count#291L])\n",
      "            +- *(2) Project [FLIGHT_NUMBER#15, AIRLINE#83]\n",
      "               +- *(2) BroadcastHashJoin [AIRLINE#14], [IATA_CODE#82], Inner, BuildRight\n",
      "                  :- *(2) Project [AIRLINE#14, FLIGHT_NUMBER#15]\n",
      "                  :  +- *(2) Filter ((isnotnull(CANCELLED#34) && (CANCELLED#34 = 1)) && isnotnull(AIRLINE#14))\n",
      "                  :     +- *(2) FileScan csv [AIRLINE#14,FLIGHT_NUMBER#15,CANCELLED#34] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/ak/code/datos/datos-spark-lesson/data/flight-delays/flights.csv], PartitionFilters: [], PushedFilters: [IsNotNull(CANCELLED), EqualTo(CANCELLED,1), IsNotNull(AIRLINE)], ReadSchema: struct<AIRLINE:string,FLIGHT_NUMBER:int,CANCELLED:int>\n",
      "                  +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      "                     +- *(1) Project [IATA_CODE#82, AIRLINE#83]\n",
      "                        +- *(1) Filter isnotnull(IATA_CODE#82)\n",
      "                           +- *(1) FileScan csv [IATA_CODE#82,AIRLINE#83] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/ak/code/datos/datos-spark-lesson/data/flight-delays/airlines.csv], PartitionFilters: [], PushedFilters: [IsNotNull(IATA_CODE)], ReadSchema: struct<IATA_CODE:string,AIRLINE:string>\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[pyspark.sql.types.Row]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql.rdd.map(lambda a: type(a)).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducción a Catalyst Optimizer\n",
    "\n",
    "En esta sección vamos a intentar profundizar como funciona el Catalyst Optimizer. Este componente del framework de ejecución se encarga de interpretar y optimizar, la query planteada, para despues llevarla a codigo en ejecución en el API de RDD.\n",
    "\n",
    "El pipeline de ejecución de **Catalyst Optimizer** se puede ver en el siguiente grafico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![title](img/catalyst-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "De cada etapa se obtienen los siguientes resultados:\n",
    "\n",
    "- Parsed/Unresolved Logical Plan, sin haber resuelto las tablas y atributos de la query.\n",
    "- Analyzed Logical Plan, luego de analizarlo o resolver las referencias de catalogo.\n",
    "- Optimized Logical Plan, luego del proceso de optimizacion logica a partir de un set de reglas.\n",
    "- Physical Plan, obteniendo el physical plan seleccionado luego de haber evaluado multiples a traves de un modelo de costos.\n",
    "\n",
    "Por ultimo, a partir del physical plan seleccionado se genera el codigo de ejecucion sobre el api de RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Entendiendo Catalyst Optimizer a partir del query plan\n",
    "\n",
    "Como un primer paso para poder entender como funciona, y como recomendacion general en todos los casos, es importante antes de ejecutar queries analizar el query plan a ejecutarse, esto se puede lograr mediante el metodo ```explain```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['AIRLINE, 'FLIGHT_NUMBER]\n",
      "+- 'Filter ('CANCELLED = 1)\n",
      "   +- 'UnresolvedRelation `flights`\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "AIRLINE: string, FLIGHT_NUMBER: int\n",
      "Project [AIRLINE#14, FLIGHT_NUMBER#15]\n",
      "+- Filter (CANCELLED#34 = 1)\n",
      "   +- SubqueryAlias `flights`\n",
      "      +- Relation[YEAR#10,MONTH#11,DAY#12,DAY_OF_WEEK#13,AIRLINE#14,FLIGHT_NUMBER#15,TAIL_NUMBER#16,ORIGIN_AIRPORT#17,DESTINATION_AIRPORT#18,SCHEDULED_DEPARTURE#19,DEPARTURE_TIME#20,DEPARTURE_DELAY#21,TAXI_OUT#22,WHEELS_OFF#23,SCHEDULED_TIME#24,ELAPSED_TIME#25,AIR_TIME#26,DISTANCE#27,WHEELS_ON#28,TAXI_IN#29,SCHEDULED_ARRIVAL#30,ARRIVAL_TIME#31,ARRIVAL_DELAY#32,DIVERTED#33,... 7 more fields] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [AIRLINE#14, FLIGHT_NUMBER#15]\n",
      "+- Filter (isnotnull(CANCELLED#34) && (CANCELLED#34 = 1))\n",
      "   +- Relation[YEAR#10,MONTH#11,DAY#12,DAY_OF_WEEK#13,AIRLINE#14,FLIGHT_NUMBER#15,TAIL_NUMBER#16,ORIGIN_AIRPORT#17,DESTINATION_AIRPORT#18,SCHEDULED_DEPARTURE#19,DEPARTURE_TIME#20,DEPARTURE_DELAY#21,TAXI_OUT#22,WHEELS_OFF#23,SCHEDULED_TIME#24,ELAPSED_TIME#25,AIR_TIME#26,DISTANCE#27,WHEELS_ON#28,TAXI_IN#29,SCHEDULED_ARRIVAL#30,ARRIVAL_TIME#31,ARRIVAL_DELAY#32,DIVERTED#33,... 7 more fields] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [AIRLINE#14, FLIGHT_NUMBER#15]\n",
      "+- *(1) Filter (isnotnull(CANCELLED#34) && (CANCELLED#34 = 1))\n",
      "   +- *(1) FileScan csv [AIRLINE#14,FLIGHT_NUMBER#15,CANCELLED#34] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/ak/code/datos/datos-spark-lesson/data/flight-delays/flights.csv], PartitionFilters: [], PushedFilters: [IsNotNull(CANCELLED), EqualTo(CANCELLED,1)], ReadSchema: struct<AIRLINE:string,FLIGHT_NUMBER:int,CANCELLED:int>\n"
     ]
    }
   ],
   "source": [
    "# comencemos analizando con una query simple de seleccion \n",
    "query = 'SELECT AIRLINE, FLIGHT_NUMBER from flights\\\n",
    "                        WHERE CANCELLED = 1'\n",
    "spark.sql(query).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Obteniendo el Unresolved Logical Plan\n",
    "\n",
    "```\n",
    "== Parsed Logical Plan ==\n",
    "'Project ['AIRLINE, 'FLIGHT_NUMBER]\n",
    "+- 'Filter ('CANCELLED = 1)\n",
    "   +- 'UnresolvedRelation `flights`\n",
    "```\n",
    "\n",
    "El pipeline comienza con una query SQL o una operatoria utilizando la API de DataFrame de Spark. La query es parseada y resuelta en Parsed Logical Plan. Este como podemos ver **tiene relaciones no resueltas aun dado que el framework todavia no sabe nada sobre los atributos y tablas en la query.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Obteniendo el Analyzed Logical Plan\n",
    "\n",
    "```\n",
    "== Analyzed Logical Plan ==\n",
    "AIRLINE: string, FLIGHT_NUMBER: int\n",
    "Project [AIRLINE#3744, FLIGHT_NUMBER#3745]\n",
    "+- Filter (CANCELLED#3764 = 1)\n",
    "   +- SubqueryAlias flights\n",
    "      +- Relation[YEAR#3740,MONTH#3741,DAY#3742,DAY_OF_WEEK#3743,AIRLINE#3744,FLIGHT_NUMBER#3745,TAIL_NUMBER#3746,ORIGIN_AIRPORT#3747,DESTINATION_AIRPORT#3748,SCHEDULED_DEPARTURE#3749,DEPARTURE_TIME#3750,DEPARTURE_DELAY#3751,TAXI_OUT#3752,WHEELS_OFF#3753,SCHEDULED_TIME#3754,ELAPSED_TIME#3755,AIR_TIME#3756,DISTANCE#3757,WHEELS_ON#3758,TAXI_IN#3759,SCHEDULED_ARRIVAL#3760,ARRIVAL_TIME#3761,ARRIVAL_DELAY#3762,DIVERTED#3763,... 7 more fields] csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Para obtener el Analyzed Logical Plan Catalyst utiliza el catalogo correspondiente a partir de los origenes de donde se ha generado el dataframe en cuestion. Por ejemplo en nuestro caso estara utilizando el **metastore de Apache Derby local**, pero tambien podria utilizar:\n",
    "\n",
    "- Si trabajaramos con **Apache Hive**, su metastore.\n",
    "- Un metastore de HDFS\n",
    "- Informacion de DBMS si estuvieramos trabajando con conexiones a base de datos relacionales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Obteniendo el Optimized Logical Plan\n",
    "\n",
    "```\n",
    "== Optimized Logical Plan ==\n",
    "Project [AIRLINE#3744, FLIGHT_NUMBER#3745]\n",
    "+- Filter (isnotnull(CANCELLED#3764) && (CANCELLED#3764 = 1))\n",
    "   +- Relation[YEAR#3740,MONTH#3741,DAY#3742,DAY_OF_WEEK#3743,AIRLINE#3744,FLIGHT_NUMBER#3745,TAIL_NUMBER#3746,ORIGIN_AIRPORT#3747,DESTINATION_AIRPORT#3748,SCHEDULED_DEPARTURE#3749,DEPARTURE_TIME#3750,DEPARTURE_DELAY#3751,TAXI_OUT#3752,WHEELS_OFF#3753,SCHEDULED_TIME#3754,ELAPSED_TIME#3755,AIR_TIME#3756,DISTANCE#3757,WHEELS_ON#3758,TAXI_IN#3759,SCHEDULED_ARRIVAL#3760,ARRIVAL_TIME#3761,ARRIVAL_DELAY#3762,DIVERTED#3763,... 7 more fields] csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Una vez que el plan esta resuelto Catalyst comienza a optimizar este plan, generando un Optimized Logical Plan.\n",
    "\n",
    "Vamos a introducir una simplificacion para entender como los planes son transformados:\n",
    "\n",
    "En general para poder hacer las optimizaciones, **Catalyst utiliza una representacion para representar la query como un arbol inmutable.** Es por eso que **la mayoria de las optimizaciones se realizan analizando la posibilidad de poder aplicar transformaciones funcionales al arbol para poder lograr los mismos resultados.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![title](img/tree-optimize.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En el grafico que tenemos arriba se ve una caso de una posible optimizacion que se podria realizar conocida como **Filter pushdown**. Esencialmente en este caso la optimizacion consiste aplicar el filtro antes para luego tener que realizar un join con menor cantidad de datos. Veremos este ejemplo en la siguiente query a analizar.\n",
    "\n",
    "Algunos otros ejemplos son:\n",
    "\n",
    "- Constant Folding\n",
    "- Predicate Pushdown (filter es un ejemplo)\n",
    "- Projection Pruning\n",
    "- Null Propagation\n",
    "- Boolean Expression Simplification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Obteniendo el Physical Plan a ejecutarse\n",
    "\n",
    "```\n",
    "== Physical Plan ==\n",
    "*(1) Project [AIRLINE#3744, FLIGHT_NUMBER#3745]\n",
    "+- *(1) Filter (isnotnull(CANCELLED#3764) && (CANCELLED#3764 = 1))\n",
    "   +- *(1) FileScan csv [AIRLINE#3744,FLIGHT_NUMBER#3745,CANCELLED#3764] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/ak/code/datos/datos-spark-lesson/data/flight-delays/flights.csv], PartitionFilters: [], PushedFilters: [IsNotNull(CANCELLED), EqualTo(CANCELLED,1)], ReadSchema: struct<AIRLINE:string,FLIGHT_NUMBER:int,CANCELLED:int>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Una vez obtenido especificamente el Optimized Logical Plan, Catalyst va a generar un conjunto de Physical Plan los cuales son evaluados con un modelo de costos, de ser necesario. Finalmente el mas performante segun el modelo de costo, sera el plan usado para generar el codigo executable via RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analizando los distintos tipos de JOINS\n",
    "\n",
    "Tomando la siguiente query con un INNER JOIN podemos analizar su query plan en los distintos stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['airlines.AIRLINE, 'flights.FLIGHT_NUMBER]\n",
      "+- 'Filter ('flights.CANCELLED = 0)\n",
      "   +- 'Join Inner, ('flights.AIRLINE = 'airlines.IATA_CODE)\n",
      "      :- 'UnresolvedRelation `flights`\n",
      "      +- 'UnresolvedRelation `airlines`\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "AIRLINE: string, FLIGHT_NUMBER: int\n",
      "Project [AIRLINE#83, FLIGHT_NUMBER#15]\n",
      "+- Filter (CANCELLED#34 = 0)\n",
      "   +- Join Inner, (AIRLINE#14 = IATA_CODE#82)\n",
      "      :- SubqueryAlias `flights`\n",
      "      :  +- Relation[YEAR#10,MONTH#11,DAY#12,DAY_OF_WEEK#13,AIRLINE#14,FLIGHT_NUMBER#15,TAIL_NUMBER#16,ORIGIN_AIRPORT#17,DESTINATION_AIRPORT#18,SCHEDULED_DEPARTURE#19,DEPARTURE_TIME#20,DEPARTURE_DELAY#21,TAXI_OUT#22,WHEELS_OFF#23,SCHEDULED_TIME#24,ELAPSED_TIME#25,AIR_TIME#26,DISTANCE#27,WHEELS_ON#28,TAXI_IN#29,SCHEDULED_ARRIVAL#30,ARRIVAL_TIME#31,ARRIVAL_DELAY#32,DIVERTED#33,... 7 more fields] csv\n",
      "      +- SubqueryAlias `airlines`\n",
      "         +- Relation[IATA_CODE#82,AIRLINE#83] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [AIRLINE#83, FLIGHT_NUMBER#15]\n",
      "+- Join Inner, (AIRLINE#14 = IATA_CODE#82)\n",
      "   :- Project [AIRLINE#14, FLIGHT_NUMBER#15]\n",
      "   :  +- Filter ((isnotnull(CANCELLED#34) && (CANCELLED#34 = 0)) && isnotnull(AIRLINE#14))\n",
      "   :     +- Relation[YEAR#10,MONTH#11,DAY#12,DAY_OF_WEEK#13,AIRLINE#14,FLIGHT_NUMBER#15,TAIL_NUMBER#16,ORIGIN_AIRPORT#17,DESTINATION_AIRPORT#18,SCHEDULED_DEPARTURE#19,DEPARTURE_TIME#20,DEPARTURE_DELAY#21,TAXI_OUT#22,WHEELS_OFF#23,SCHEDULED_TIME#24,ELAPSED_TIME#25,AIR_TIME#26,DISTANCE#27,WHEELS_ON#28,TAXI_IN#29,SCHEDULED_ARRIVAL#30,ARRIVAL_TIME#31,ARRIVAL_DELAY#32,DIVERTED#33,... 7 more fields] csv\n",
      "   +- Filter isnotnull(IATA_CODE#82)\n",
      "      +- Relation[IATA_CODE#82,AIRLINE#83] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) Project [AIRLINE#83, FLIGHT_NUMBER#15]\n",
      "+- *(2) BroadcastHashJoin [AIRLINE#14], [IATA_CODE#82], Inner, BuildRight\n",
      "   :- *(2) Project [AIRLINE#14, FLIGHT_NUMBER#15]\n",
      "   :  +- *(2) Filter ((isnotnull(CANCELLED#34) && (CANCELLED#34 = 0)) && isnotnull(AIRLINE#14))\n",
      "   :     +- *(2) FileScan csv [AIRLINE#14,FLIGHT_NUMBER#15,CANCELLED#34] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/ak/code/datos/datos-spark-lesson/data/flight-delays/flights.csv], PartitionFilters: [], PushedFilters: [IsNotNull(CANCELLED), EqualTo(CANCELLED,0), IsNotNull(AIRLINE)], ReadSchema: struct<AIRLINE:string,FLIGHT_NUMBER:int,CANCELLED:int>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      "      +- *(1) Project [IATA_CODE#82, AIRLINE#83]\n",
      "         +- *(1) Filter isnotnull(IATA_CODE#82)\n",
      "            +- *(1) FileScan csv [IATA_CODE#82,AIRLINE#83] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/ak/code/datos/datos-spark-lesson/data/flight-delays/airlines.csv], PartitionFilters: [], PushedFilters: [IsNotNull(IATA_CODE)], ReadSchema: struct<IATA_CODE:string,AIRLINE:string>\n"
     ]
    }
   ],
   "source": [
    "query = 'SELECT airlines.AIRLINE, flights.FLIGHT_NUMBER\\\n",
    "                FROM flights INNER JOIN airlines\\\n",
    "                ON flights.AIRLINE = airlines.IATA_CODE\\\n",
    "                WHERE flights.CANCELLED = 0'\n",
    "\n",
    "spark.sql(query).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "== Parsed Logical Plan ==\n",
    "'Project ['airlines.AIRLINE, 'flights.FLIGHT_NUMBER]\n",
    "+- 'Filter ('flights.CANCELLED = 0)\n",
    "   +- 'Join Inner, ('flights.AIRLINE = 'airlines.IATA_CODE)\n",
    "      :- 'UnresolvedRelation `flights`\n",
    "      +- 'UnresolvedRelation `airlines`\n",
    "\n",
    "== Analyzed Logical Plan ==\n",
    "AIRLINE: string, FLIGHT_NUMBER: int\n",
    "Project [AIRLINE#83, FLIGHT_NUMBER#15]\n",
    "+- Filter (CANCELLED#34 = 0)\n",
    "   +- Join Inner, (AIRLINE#14 = IATA_CODE#82)\n",
    "      :- SubqueryAlias `flights`\n",
    "      :  +- Relation[YEAR#10,MONTH#11,DAY#12,DAY_OF_WEEK#13,AIRLINE#14,FLIGHT_NUMBER#15,TAIL_NUMBER#16,ORIGIN_AIRPORT#17,DESTINATION_AIRPORT#18,SCHEDULED_DEPARTURE#19,DEPARTURE_TIME#20,DEPARTURE_DELAY#21,TAXI_OUT#22,WHEELS_OFF#23,SCHEDULED_TIME#24,ELAPSED_TIME#25,AIR_TIME#26,DISTANCE#27,WHEELS_ON#28,TAXI_IN#29,SCHEDULED_ARRIVAL#30,ARRIVAL_TIME#31,ARRIVAL_DELAY#32,DIVERTED#33,... 7 more fields] csv\n",
    "      +- SubqueryAlias `airlines`\n",
    "         +- Relation[IATA_CODE#82,AIRLINE#83] csv\n",
    "\n",
    "== Optimized Logical Plan ==\n",
    "Project [AIRLINE#83, FLIGHT_NUMBER#15]\n",
    "+- Join Inner, (AIRLINE#14 = IATA_CODE#82)\n",
    "   :- Project [AIRLINE#14, FLIGHT_NUMBER#15]\n",
    "   :  +- Filter ((isnotnull(CANCELLED#34) && (CANCELLED#34 = 0)) && isnotnull(AIRLINE#14))\n",
    "   :     +- Relation[YEAR#10,MONTH#11,DAY#12,DAY_OF_WEEK#13,AIRLINE#14,FLIGHT_NUMBER#15,TAIL_NUMBER#16,ORIGIN_AIRPORT#17,DESTINATION_AIRPORT#18,SCHEDULED_DEPARTURE#19,DEPARTURE_TIME#20,DEPARTURE_DELAY#21,TAXI_OUT#22,WHEELS_OFF#23,SCHEDULED_TIME#24,ELAPSED_TIME#25,AIR_TIME#26,DISTANCE#27,WHEELS_ON#28,TAXI_IN#29,SCHEDULED_ARRIVAL#30,ARRIVAL_TIME#31,ARRIVAL_DELAY#32,DIVERTED#33,... 7 more fields] csv\n",
    "   +- Filter isnotnull(IATA_CODE#82)\n",
    "      +- Relation[IATA_CODE#82,AIRLINE#83] csv\n",
    "\n",
    "== Physical Plan ==\n",
    "*(2) Project [AIRLINE#83, FLIGHT_NUMBER#15]\n",
    "+- *(2) BroadcastHashJoin [AIRLINE#14], [IATA_CODE#82], Inner, BuildRight\n",
    "   :- *(2) Project [AIRLINE#14, FLIGHT_NUMBER#15]\n",
    "   :  +- *(2) Filter ((isnotnull(CANCELLED#34) && (CANCELLED#34 = 0)) && isnotnull(AIRLINE#14))\n",
    "   :     +- *(2) FileScan csv [AIRLINE#14,FLIGHT_NUMBER#15,CANCELLED#34] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/ak/code/datos/datos-spark-lesson/data/flight-delays/flights.csv], PartitionFilters: [], PushedFilters: [IsNotNull(CANCELLED), EqualTo(CANCELLED,0), IsNotNull(AIRLINE)], ReadSchema: struct<AIRLINE:string,FLIGHT_NUMBER:int,CANCELLED:int>\n",
    "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
    "      +- *(1) Project [IATA_CODE#82, AIRLINE#83]\n",
    "         +- *(1) Filter isnotnull(IATA_CODE#82)\n",
    "            +- *(1) FileScan csv [IATA_CODE#82,AIRLINE#83] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/ak/code/datos/datos-spark-lesson/data/flight-delays/airlines.csv], PartitionFilters: [], PushedFilters: [IsNotNull(IATA_CODE)], ReadSchema: struct<IATA_CODE:string,AIRLINE:string>\n",
    "```            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['airlines.AIRLINE, 'flights.FLIGHT_NUMBER]\n",
      "+- 'Filter ('flights.CANCELLED = 0)\n",
      "   +- 'Join Inner, ('airlines.IATA_CODE = 'flights.AIRLINE)\n",
      "      :- 'UnresolvedRelation `airlines`\n",
      "      +- 'UnresolvedRelation `flights`\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "AIRLINE: string, FLIGHT_NUMBER: int\n",
      "Project [AIRLINE#83, FLIGHT_NUMBER#15]\n",
      "+- Filter (CANCELLED#34 = 0)\n",
      "   +- Join Inner, (IATA_CODE#82 = AIRLINE#14)\n",
      "      :- SubqueryAlias `airlines`\n",
      "      :  +- Relation[IATA_CODE#82,AIRLINE#83] csv\n",
      "      +- SubqueryAlias `flights`\n",
      "         +- Relation[YEAR#10,MONTH#11,DAY#12,DAY_OF_WEEK#13,AIRLINE#14,FLIGHT_NUMBER#15,TAIL_NUMBER#16,ORIGIN_AIRPORT#17,DESTINATION_AIRPORT#18,SCHEDULED_DEPARTURE#19,DEPARTURE_TIME#20,DEPARTURE_DELAY#21,TAXI_OUT#22,WHEELS_OFF#23,SCHEDULED_TIME#24,ELAPSED_TIME#25,AIR_TIME#26,DISTANCE#27,WHEELS_ON#28,TAXI_IN#29,SCHEDULED_ARRIVAL#30,ARRIVAL_TIME#31,ARRIVAL_DELAY#32,DIVERTED#33,... 7 more fields] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [AIRLINE#83, FLIGHT_NUMBER#15]\n",
      "+- Join Inner, (IATA_CODE#82 = AIRLINE#14)\n",
      "   :- Filter isnotnull(IATA_CODE#82)\n",
      "   :  +- Relation[IATA_CODE#82,AIRLINE#83] csv\n",
      "   +- Project [AIRLINE#14, FLIGHT_NUMBER#15]\n",
      "      +- Filter ((isnotnull(CANCELLED#34) && (CANCELLED#34 = 0)) && isnotnull(AIRLINE#14))\n",
      "         +- Relation[YEAR#10,MONTH#11,DAY#12,DAY_OF_WEEK#13,AIRLINE#14,FLIGHT_NUMBER#15,TAIL_NUMBER#16,ORIGIN_AIRPORT#17,DESTINATION_AIRPORT#18,SCHEDULED_DEPARTURE#19,DEPARTURE_TIME#20,DEPARTURE_DELAY#21,TAXI_OUT#22,WHEELS_OFF#23,SCHEDULED_TIME#24,ELAPSED_TIME#25,AIR_TIME#26,DISTANCE#27,WHEELS_ON#28,TAXI_IN#29,SCHEDULED_ARRIVAL#30,ARRIVAL_TIME#31,ARRIVAL_DELAY#32,DIVERTED#33,... 7 more fields] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) Project [AIRLINE#83, FLIGHT_NUMBER#15]\n",
      "+- *(2) BroadcastHashJoin [IATA_CODE#82], [AIRLINE#14], Inner, BuildLeft\n",
      "   :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      "   :  +- *(1) Project [IATA_CODE#82, AIRLINE#83]\n",
      "   :     +- *(1) Filter isnotnull(IATA_CODE#82)\n",
      "   :        +- *(1) FileScan csv [IATA_CODE#82,AIRLINE#83] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/ak/code/datos/datos-spark-lesson/data/flight-delays/airlines.csv], PartitionFilters: [], PushedFilters: [IsNotNull(IATA_CODE)], ReadSchema: struct<IATA_CODE:string,AIRLINE:string>\n",
      "   +- *(2) Project [AIRLINE#14, FLIGHT_NUMBER#15]\n",
      "      +- *(2) Filter ((isnotnull(CANCELLED#34) && (CANCELLED#34 = 0)) && isnotnull(AIRLINE#14))\n",
      "         +- *(2) FileScan csv [AIRLINE#14,FLIGHT_NUMBER#15,CANCELLED#34] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/ak/code/datos/datos-spark-lesson/data/flight-delays/flights.csv], PartitionFilters: [], PushedFilters: [IsNotNull(CANCELLED), EqualTo(CANCELLED,0), IsNotNull(AIRLINE)], ReadSchema: struct<AIRLINE:string,FLIGHT_NUMBER:int,CANCELLED:int>\n"
     ]
    }
   ],
   "source": [
    "query = 'SELECT airlines.AIRLINE, flights.FLIGHT_NUMBER\\\n",
    "                FROM airlines INNER JOIN flights\\\n",
    "                ON airlines.IATA_CODE = flights.AIRLINE\\\n",
    "                WHERE flights.CANCELLED = 0'\n",
    "\n",
    "spark.sql(query).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "== Parsed Logical Plan ==\n",
    "'Project ['airlines.AIRLINE, 'flights.FLIGHT_NUMBER]\n",
    "+- 'Filter ('flights.CANCELLED = 0)\n",
    "   +- 'Join Inner, ('airlines.IATA_CODE = 'flights.AIRLINE)\n",
    "      :- 'UnresolvedRelation `airlines`\n",
    "      +- 'UnresolvedRelation `flights`\n",
    "\n",
    "== Analyzed Logical Plan ==\n",
    "AIRLINE: string, FLIGHT_NUMBER: int\n",
    "Project [AIRLINE#83, FLIGHT_NUMBER#15]\n",
    "+- Filter (CANCELLED#34 = 0)\n",
    "   +- Join Inner, (IATA_CODE#82 = AIRLINE#14)\n",
    "      :- SubqueryAlias `airlines`\n",
    "      :  +- Relation[IATA_CODE#82,AIRLINE#83] csv\n",
    "      +- SubqueryAlias `flights`\n",
    "         +- Relation[YEAR#10,MONTH#11,DAY#12,DAY_OF_WEEK#13,AIRLINE#14,FLIGHT_NUMBER#15,TAIL_NUMBER#16,ORIGIN_AIRPORT#17,DESTINATION_AIRPORT#18,SCHEDULED_DEPARTURE#19,DEPARTURE_TIME#20,DEPARTURE_DELAY#21,TAXI_OUT#22,WHEELS_OFF#23,SCHEDULED_TIME#24,ELAPSED_TIME#25,AIR_TIME#26,DISTANCE#27,WHEELS_ON#28,TAXI_IN#29,SCHEDULED_ARRIVAL#30,ARRIVAL_TIME#31,ARRIVAL_DELAY#32,DIVERTED#33,... 7 more fields] csv\n",
    "\n",
    "== Optimized Logical Plan ==\n",
    "Project [AIRLINE#83, FLIGHT_NUMBER#15]\n",
    "+- Join Inner, (IATA_CODE#82 = AIRLINE#14)\n",
    "   :- Filter isnotnull(IATA_CODE#82)\n",
    "   :  +- Relation[IATA_CODE#82,AIRLINE#83] csv\n",
    "   +- Project [AIRLINE#14, FLIGHT_NUMBER#15]\n",
    "      +- Filter ((isnotnull(CANCELLED#34) && (CANCELLED#34 = 0)) && isnotnull(AIRLINE#14))\n",
    "         +- Relation[YEAR#10,MONTH#11,DAY#12,DAY_OF_WEEK#13,AIRLINE#14,FLIGHT_NUMBER#15,TAIL_NUMBER#16,ORIGIN_AIRPORT#17,DESTINATION_AIRPORT#18,SCHEDULED_DEPARTURE#19,DEPARTURE_TIME#20,DEPARTURE_DELAY#21,TAXI_OUT#22,WHEELS_OFF#23,SCHEDULED_TIME#24,ELAPSED_TIME#25,AIR_TIME#26,DISTANCE#27,WHEELS_ON#28,TAXI_IN#29,SCHEDULED_ARRIVAL#30,ARRIVAL_TIME#31,ARRIVAL_DELAY#32,DIVERTED#33,... 7 more fields] csv\n",
    "\n",
    "== Physical Plan ==\n",
    "*(2) Project [AIRLINE#83, FLIGHT_NUMBER#15]\n",
    "+- *(2) BroadcastHashJoin [IATA_CODE#82], [AIRLINE#14], Inner, BuildLeft\n",
    "   :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
    "   :  +- *(1) Project [IATA_CODE#82, AIRLINE#83]\n",
    "   :     +- *(1) Filter isnotnull(IATA_CODE#82)\n",
    "   :        +- *(1) FileScan csv [IATA_CODE#82,AIRLINE#83] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/ak/code/datos/datos-spark-lesson/data/flight-delays/airlines.csv], PartitionFilters: [], PushedFilters: [IsNotNull(IATA_CODE)], ReadSchema: struct<IATA_CODE:string,AIRLINE:string>\n",
    "   +- *(2) Project [AIRLINE#14, FLIGHT_NUMBER#15]\n",
    "      +- *(2) Filter ((isnotnull(CANCELLED#34) && (CANCELLED#34 = 0)) && isnotnull(AIRLINE#14))\n",
    "         +- *(2) FileScan csv [AIRLINE#14,FLIGHT_NUMBER#15,CANCELLED#34] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/ak/code/datos/datos-spark-lesson/data/flight-delays/flights.csv], PartitionFilters: [], PushedFilters: [IsNotNull(CANCELLED), EqualTo(CANCELLED,0), IsNotNull(AIRLINE)], ReadSchema: struct<AIRLINE:string,FLIGHT_NUMBER:int,CANCELLED:int>\n",
    "```                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Algunos puntos interesantes a considerar:\n",
    "\n",
    "- Ver como se produce un **filter pushdown** en el optimized logical plan.\n",
    "- Ver como se produce un **proyeccion de las columnas necesarias** antes del join en el optimized logical plan para reducir la cantidad de datos a utilizar.\n",
    "- Ver el tipo de JOIN a realizarse: **BroadcastHashJoin**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Como pueden implementarse los Joins en forma general\n",
    "\n",
    "Existen las siguientes estrategias para implementar joins:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Nested Loop Join\n",
    "\n",
    "- Metodo Naive para implementar un join con dos loops anidados. \n",
    "- ```O(n*m)``` = ***MUY LENTO***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hash Join\n",
    "- Crear una Hashmap con la tabla mas chica del join\n",
    "- ```O(n+m)``` dado que necesita crear un Hashmap, usando memoria extra.\n",
    "- Solamente soporta comparacion a claves iguales para el join (equijoin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sort Merge Join\n",
    "- ```O(n (log(n)) + m (log(m)))```, un poco mas lento que Hash Join.\n",
    "- No hace uso de una meta estructura y puede ser usado con tablas muy grandes.\n",
    "- Las keys deben poder ordenarse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Joins en Spark y como se efectuan en el Physical Plan\n",
    "\n",
    "Existen cuatro tipos de physical plans para efectuar joins en Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sort-Merge Join\n",
    "\n",
    "- Implementacion Default de Join en Spark.\n",
    "- Keys deben poder ordenarse.\n",
    "- Produce Shuffle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Shuffle Hash Join\n",
    "\n",
    "- Utilizado cuando las keys no pueden ordenarse o el sort merge join esta deshabilitado.\n",
    "- Se usa cuando usualmente un \"lado\" es mas chico que el otro. (3 veces)\n",
    "- Spark debe ser capaz de poder crear un hashmap local con los recursos que tiene disponibles. (lo estima)\n",
    "- Podemos forzarlo si creemos que nos ayuda como mejora de performance.\n",
    "- Produce Shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Broadcast Join\n",
    "\n",
    "- No produce Shuffle, dado que realiza broadcast completo de un lado del join a todos los executors.\n",
    "- Spark debe evaluar si puede realizar el broadcast.\n",
    "- Hace broadcast del dataframe mas chico.\n",
    "- Hace el join del lado del map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Broadcast Nested Loop Join\n",
    "\n",
    "- Caso PATOLOGICO que queremos evitar.\n",
    "\n",
    "Otro aspecto importante a considerar al trabajar con joins Grandes el nivel de ```paralellism``` es decir la cantidad de particiones que utilizamos dependiendo de como estan distribuidas nuestras keys. (skewed data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analizando el query plan de Agregacion (Group By)\n",
    "\n",
    "Por ultimo veamos un ejemplo mas para ver algunos tipos mas de operaciones y como se optimizan (agregacion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['airlines.AIRLINE], ['airlines.AIRLINE, 'count('flights.FLIGHT_NUMBER) AS number_of_fligths#305]\n",
      "+- 'Filter (('flights.AIRLINE = 'airlines.IATA_CODE) && ('flights.CANCELLED = 0))\n",
      "   +- 'Join Inner\n",
      "      :- 'UnresolvedRelation `flights`\n",
      "      +- 'UnresolvedRelation `airlines`\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "AIRLINE: string, number_of_fligths: bigint\n",
      "Aggregate [AIRLINE#83], [AIRLINE#83, count(FLIGHT_NUMBER#15) AS number_of_fligths#305L]\n",
      "+- Filter ((AIRLINE#14 = IATA_CODE#82) && (CANCELLED#34 = 0))\n",
      "   +- Join Inner\n",
      "      :- SubqueryAlias `flights`\n",
      "      :  +- Relation[YEAR#10,MONTH#11,DAY#12,DAY_OF_WEEK#13,AIRLINE#14,FLIGHT_NUMBER#15,TAIL_NUMBER#16,ORIGIN_AIRPORT#17,DESTINATION_AIRPORT#18,SCHEDULED_DEPARTURE#19,DEPARTURE_TIME#20,DEPARTURE_DELAY#21,TAXI_OUT#22,WHEELS_OFF#23,SCHEDULED_TIME#24,ELAPSED_TIME#25,AIR_TIME#26,DISTANCE#27,WHEELS_ON#28,TAXI_IN#29,SCHEDULED_ARRIVAL#30,ARRIVAL_TIME#31,ARRIVAL_DELAY#32,DIVERTED#33,... 7 more fields] csv\n",
      "      +- SubqueryAlias `airlines`\n",
      "         +- Relation[IATA_CODE#82,AIRLINE#83] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [AIRLINE#83], [AIRLINE#83, count(FLIGHT_NUMBER#15) AS number_of_fligths#305L]\n",
      "+- Project [FLIGHT_NUMBER#15, AIRLINE#83]\n",
      "   +- Join Inner, (AIRLINE#14 = IATA_CODE#82)\n",
      "      :- Project [AIRLINE#14, FLIGHT_NUMBER#15]\n",
      "      :  +- Filter ((isnotnull(CANCELLED#34) && (CANCELLED#34 = 0)) && isnotnull(AIRLINE#14))\n",
      "      :     +- Relation[YEAR#10,MONTH#11,DAY#12,DAY_OF_WEEK#13,AIRLINE#14,FLIGHT_NUMBER#15,TAIL_NUMBER#16,ORIGIN_AIRPORT#17,DESTINATION_AIRPORT#18,SCHEDULED_DEPARTURE#19,DEPARTURE_TIME#20,DEPARTURE_DELAY#21,TAXI_OUT#22,WHEELS_OFF#23,SCHEDULED_TIME#24,ELAPSED_TIME#25,AIR_TIME#26,DISTANCE#27,WHEELS_ON#28,TAXI_IN#29,SCHEDULED_ARRIVAL#30,ARRIVAL_TIME#31,ARRIVAL_DELAY#32,DIVERTED#33,... 7 more fields] csv\n",
      "      +- Filter isnotnull(IATA_CODE#82)\n",
      "         +- Relation[IATA_CODE#82,AIRLINE#83] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) HashAggregate(keys=[AIRLINE#83], functions=[count(FLIGHT_NUMBER#15)], output=[AIRLINE#83, number_of_fligths#305L])\n",
      "+- Exchange hashpartitioning(AIRLINE#83, 200)\n",
      "   +- *(2) HashAggregate(keys=[AIRLINE#83], functions=[partial_count(FLIGHT_NUMBER#15)], output=[AIRLINE#83, count#310L])\n",
      "      +- *(2) Project [FLIGHT_NUMBER#15, AIRLINE#83]\n",
      "         +- *(2) BroadcastHashJoin [AIRLINE#14], [IATA_CODE#82], Inner, BuildRight\n",
      "            :- *(2) Project [AIRLINE#14, FLIGHT_NUMBER#15]\n",
      "            :  +- *(2) Filter ((isnotnull(CANCELLED#34) && (CANCELLED#34 = 0)) && isnotnull(AIRLINE#14))\n",
      "            :     +- *(2) FileScan csv [AIRLINE#14,FLIGHT_NUMBER#15,CANCELLED#34] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/ak/code/datos/datos-spark-lesson/data/flight-delays/flights.csv], PartitionFilters: [], PushedFilters: [IsNotNull(CANCELLED), EqualTo(CANCELLED,0), IsNotNull(AIRLINE)], ReadSchema: struct<AIRLINE:string,FLIGHT_NUMBER:int,CANCELLED:int>\n",
      "            +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      "               +- *(1) Project [IATA_CODE#82, AIRLINE#83]\n",
      "                  +- *(1) Filter isnotnull(IATA_CODE#82)\n",
      "                     +- *(1) FileScan csv [IATA_CODE#82,AIRLINE#83] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/ak/code/datos/datos-spark-lesson/data/flight-delays/airlines.csv], PartitionFilters: [], PushedFilters: [IsNotNull(IATA_CODE)], ReadSchema: struct<IATA_CODE:string,AIRLINE:string>\n"
     ]
    }
   ],
   "source": [
    "query = 'SELECT airlines.AIRLINE,\\\n",
    "                count(flights.FLIGHT_NUMBER) as number_of_fligths\\\n",
    "                FROM flights JOIN airlines\\\n",
    "                WHERE flights.AIRLINE = airlines.IATA_CODE\\\n",
    "                AND flights.CANCELLED = 0\\\n",
    "                GROUP BY airlines.AIRLINE'\n",
    "\n",
    "spark.sql(query).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "== Parsed Logical Plan ==\n",
    "'Aggregate ['airlines.AIRLINE], ['airlines.AIRLINE, 'count('flights.FLIGHT_NUMBER) AS number_of_fligths#305]\n",
    "+- 'Filter (('flights.AIRLINE = 'airlines.IATA_CODE) && ('flights.CANCELLED = 0))\n",
    "   +- 'Join Inner\n",
    "      :- 'UnresolvedRelation `flights`\n",
    "      +- 'UnresolvedRelation `airlines`\n",
    "\n",
    "== Analyzed Logical Plan ==\n",
    "AIRLINE: string, number_of_fligths: bigint\n",
    "Aggregate [AIRLINE#83], [AIRLINE#83, count(FLIGHT_NUMBER#15) AS number_of_fligths#305L]\n",
    "+- Filter ((AIRLINE#14 = IATA_CODE#82) && (CANCELLED#34 = 0))\n",
    "   +- Join Inner\n",
    "      :- SubqueryAlias `flights`\n",
    "      :  +- Relation[YEAR#10,MONTH#11,DAY#12,DAY_OF_WEEK#13,AIRLINE#14,FLIGHT_NUMBER#15,TAIL_NUMBER#16,ORIGIN_AIRPORT#17,DESTINATION_AIRPORT#18,SCHEDULED_DEPARTURE#19,DEPARTURE_TIME#20,DEPARTURE_DELAY#21,TAXI_OUT#22,WHEELS_OFF#23,SCHEDULED_TIME#24,ELAPSED_TIME#25,AIR_TIME#26,DISTANCE#27,WHEELS_ON#28,TAXI_IN#29,SCHEDULED_ARRIVAL#30,ARRIVAL_TIME#31,ARRIVAL_DELAY#32,DIVERTED#33,... 7 more fields] csv\n",
    "      +- SubqueryAlias `airlines`\n",
    "         +- Relation[IATA_CODE#82,AIRLINE#83] csv\n",
    "\n",
    "== Optimized Logical Plan ==\n",
    "Aggregate [AIRLINE#83], [AIRLINE#83, count(FLIGHT_NUMBER#15) AS number_of_fligths#305L]\n",
    "+- Project [FLIGHT_NUMBER#15, AIRLINE#83]\n",
    "   +- Join Inner, (AIRLINE#14 = IATA_CODE#82)\n",
    "      :- Project [AIRLINE#14, FLIGHT_NUMBER#15]\n",
    "      :  +- Filter ((isnotnull(CANCELLED#34) && (CANCELLED#34 = 0)) && isnotnull(AIRLINE#14))\n",
    "      :     +- Relation[YEAR#10,MONTH#11,DAY#12,DAY_OF_WEEK#13,AIRLINE#14,FLIGHT_NUMBER#15,TAIL_NUMBER#16,ORIGIN_AIRPORT#17,DESTINATION_AIRPORT#18,SCHEDULED_DEPARTURE#19,DEPARTURE_TIME#20,DEPARTURE_DELAY#21,TAXI_OUT#22,WHEELS_OFF#23,SCHEDULED_TIME#24,ELAPSED_TIME#25,AIR_TIME#26,DISTANCE#27,WHEELS_ON#28,TAXI_IN#29,SCHEDULED_ARRIVAL#30,ARRIVAL_TIME#31,ARRIVAL_DELAY#32,DIVERTED#33,... 7 more fields] csv\n",
    "      +- Filter isnotnull(IATA_CODE#82)\n",
    "         +- Relation[IATA_CODE#82,AIRLINE#83] csv\n",
    "\n",
    "== Physical Plan ==\n",
    "*(3) HashAggregate(keys=[AIRLINE#83], functions=[count(FLIGHT_NUMBER#15)], output=[AIRLINE#83, number_of_fligths#305L])\n",
    "+- Exchange hashpartitioning(AIRLINE#83, 200)\n",
    "   +- *(2) HashAggregate(keys=[AIRLINE#83], functions=[partial_count(FLIGHT_NUMBER#15)], output=[AIRLINE#83, count#310L])\n",
    "      +- *(2) Project [FLIGHT_NUMBER#15, AIRLINE#83]\n",
    "         +- *(2) BroadcastHashJoin [AIRLINE#14], [IATA_CODE#82], Inner, BuildRight\n",
    "            :- *(2) Project [AIRLINE#14, FLIGHT_NUMBER#15]\n",
    "            :  +- *(2) Filter ((isnotnull(CANCELLED#34) && (CANCELLED#34 = 0)) && isnotnull(AIRLINE#14))\n",
    "            :     +- *(2) FileScan csv [AIRLINE#14,FLIGHT_NUMBER#15,CANCELLED#34] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/ak/code/datos/datos-spark-lesson/data/flight-delays/flights.csv], PartitionFilters: [], PushedFilters: [IsNotNull(CANCELLED), EqualTo(CANCELLED,0), IsNotNull(AIRLINE)], ReadSchema: struct<AIRLINE:string,FLIGHT_NUMBER:int,CANCELLED:int>\n",
    "            +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
    "               +- *(1) Project [IATA_CODE#82, AIRLINE#83]\n",
    "                  +- *(1) Filter isnotnull(IATA_CODE#82)\n",
    "                     +- *(1) FileScan csv [IATA_CODE#82,AIRLINE#83] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/ak/code/datos/datos-spark-lesson/data/flight-delays/airlines.csv], PartitionFilters: [], PushedFilters: [IsNotNull(IATA_CODE)], ReadSchema: struct<IATA_CODE:string,AIRLINE:string>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Algunos puntos interesantes a considerar:\n",
    "\n",
    "- Ver como se produce un **filter pushdown** en el optimized logical plan.\n",
    "- Ver como se produce un **proyeccion de las columnas necesarias** antes del join en el optimized logical plan para reducir la cantidad de datos a utilizar.\n",
    "- Ver el tipo de JOIN a realizarse: **BroadcastHashJoin**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aspectos importantes al Utilizar SparkSQL:\n",
    "\n",
    "- **SIEMPRE** verificar el **plan de ejecucion** producido por el **explain** method.\n",
    "- Verificar si los filtros bajan en el arbol (filter push down), de tal forma que se reduzcan los set de datos a operar.\n",
    "- Verificar COMO se va a realizar el JOIN en el Physical Plan.\n",
    "- Verificar si columnas innecesarias fueron eliminas en las etapas de la relacion, de tal forma que reduzca los sets de datos a operar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Definir operaciones sin necesidad de SQL usando el API\n",
    "\n",
    "Existe la posibilidad de trabajar con dataframes de una forma bastante similar a la con la que trabajamos en pandas. A continuacion algunos ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             AIRLINE|\n",
      "+--------------------+\n",
      "|United Air Lines ...|\n",
      "|American Airlines...|\n",
      "|     US Airways Inc.|\n",
      "|Frontier Airlines...|\n",
      "|     JetBlue Airways|\n",
      "|Skywest Airlines ...|\n",
      "|Alaska Airlines Inc.|\n",
      "|    Spirit Air Lines|\n",
      "|Southwest Airline...|\n",
      "|Delta Air Lines Inc.|\n",
      "|Atlantic Southeas...|\n",
      "|Hawaiian Airlines...|\n",
      "|American Eagle Ai...|\n",
      "|      Virgin America|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_a.select(\"AIRLINE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|DAY_OF_WEEK|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          2|\n",
      "|          3|\n",
      "|          4|\n",
      "|          5|\n",
      "|          6|\n",
      "|          7|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_f.select('DAY_OF_WEEK').distinct().orderBy('DAY_OF_WEEK').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|YEAR|  count|\n",
      "+----+-------+\n",
      "|2015|5819079|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_f.groupBy(\"YEAR\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|YEAR|MONTH| count|\n",
      "+----+-----+------+\n",
      "|2015|    1|469968|\n",
      "|2015|    2|429191|\n",
      "|2015|    3|504312|\n",
      "|2015|    4|485151|\n",
      "|2015|    5|496993|\n",
      "|2015|    6|503897|\n",
      "|2015|    7|520718|\n",
      "|2015|    8|510536|\n",
      "|2015|    9|464946|\n",
      "|2015|   10|486165|\n",
      "|2015|   11|467972|\n",
      "|2015|   12|479230|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_f.groupBy(\"YEAR\",\"MONTH\").count().orderBy(\"YEAR\",\"MONTH\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algunos puntos interesantes a desarrollar e investigar\n",
    "\n",
    "- Interoperabilidad con RDDs (usando reflection o especificando el esquema).\n",
    "- Uso y Optimizacion de UDF \n",
    "- Manejo de DataSources de distintos tipos (JSON, ORC, Parquet).\n",
    "- Save Modes\n",
    "- Interoperabilidad con Apache Hive.\n",
    "- Interoperabilidad con Pandas.\n",
    "- Interoperabilidad con fuentes JDBC.\n",
    "- Distributed SQL Engine (Hola Presto)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
