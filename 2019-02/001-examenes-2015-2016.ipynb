{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2015-02 2do Recuperatorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un telescopio registra automaticamente la luminosidad de distintas\n",
    "estrellas generando un RDD con registros de tipo (star_id,\n",
    "magnitude,spectral_type, timestamp). Queremos obtener un listado de\n",
    "estrellas que tienen el mismo tipo espectral e igual promedio de\n",
    "magnitud una vez redondeado el mismo a un decimal. El resultado\n",
    "debe ser una lista en donde cada elemento de la lista es una lista de ids\n",
    "de estrellas similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars = [\n",
    "    (1, 5, 1, 1),\n",
    "    (2, 10, 1, 1),\n",
    "    (3, 6, 1, 1),\n",
    "    (4, 5.5, 2, 1),\n",
    "    (1, 6, 1, 2),\n",
    "    (2, 9, 1, 2),\n",
    "    (3, 5, 1, 2),\n",
    "    (1, 5.5, 1, 3),\n",
    "    (2, 11, 1, 3),\n",
    "    (3, 5.5, 1, 3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize(stars);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 5, 1, 1),\n",
       " (2, 10, 1, 1),\n",
       " (3, 6, 1, 1),\n",
       " (4, 5.5, 2, 1),\n",
       " (1, 6, 1, 2),\n",
       " (2, 9, 1, 2),\n",
       " (3, 5, 1, 2),\n",
       " (1, 5.5, 1, 3),\n",
       " (2, 11, 1, 3),\n",
       " (3, 5.5, 1, 3)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 5.5), [1, 3]), ((1, 10.0), [2]), ((2, 5.5), [4])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.map(lambda x: (x[0], (x[2], x[1], 1)))\\\n",
    "    .reduceByKey(lambda x, y: (x[0], x[1]+y[1], x[2]+y[2]))\\\n",
    "    .map(lambda x: ((x[1][0], x[1][1]/x[1][2]), x[0]))\\\n",
    "    .groupByKey()\\\n",
    "    .map(lambda x: (x[0], list(x[1])))\\\n",
    "    .collect()\n",
    "    \n",
    "# El último map es simplemente para convertir la sequencia de spark en una lista de python para poder verla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2015-01 2do Recuperatorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos una colección de documentos (textos) almacenados en un cluster. Se quiere establecer un ranking de los patrones mas frecuentes para la aparición de letras en las palabras. Por ejemplo “sister”, “ejects” , “ninety” y “amazon” responden al patrón “abacde”. Programar en map-reduce un programa que genere como resultado un\n",
    "listado de tipo (patron, frecuencia) indicando cuántas veces aparece cada patrón en la colección de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1609', 'the', 'sonnets', 'by', 'william']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile = sc.textFile(\"data/shakespeare.txt\")\n",
    "words = textFile.flatMap(lambda line: line.split()).map(lambda word: word.lower())\n",
    "words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abc', 165274),\n",
       " ('abcd', 146402),\n",
       " ('ab', 138768),\n",
       " ('abcde', 83840),\n",
       " ('abcdef', 47396),\n",
       " ('a', 35512),\n",
       " ('abcdefg', 26066),\n",
       " ('abca', 13868),\n",
       " ('abcdefgh', 13800),\n",
       " ('abcc', 13287)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pattern(word):\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    pat = ''\n",
    "    found = ''\n",
    "    for letter in word:\n",
    "        if letter in found:\n",
    "            pat += pat[found.index(letter)]\n",
    "        else:\n",
    "            found += letter\n",
    "            if len(found) > len(letters):\n",
    "                pat += '?'\n",
    "            else:\n",
    "                pat += letters[len(found)-1]\n",
    "    return pat\n",
    "   \n",
    "words.map(lambda x: (pattern(x), 1))\\\n",
    "     .reduceByKey(lambda x, y: x+y)\\\n",
    "     .takeOrdered(10, lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2016-02 Parcial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio queremos programar un sistema que recomiende textos a usuarios en base a sus gustos sobre ciertos términos (palabras).\n",
    "\n",
    "Se cuenta con un RDD de textos de la forma (docId, texto) donde texto es un string de longitud variable.\n",
    "\n",
    "Además contamos con un RDD que indica qué términos le gustan o no a cada usuario de la forma (userId, término, score) por ejemplo (23, “calesita”, -2).\n",
    "\n",
    "Se pide programar en Spark un programa que calcule el score total de cada documento para cada usuario generando un RDD de la forma (userId, docId, score) en donde el score es simplemente la suma de los scores del usuario para los términos que aparecen en el documento.\n",
    "\n",
    "Puede haber términos en los documentos para los cuales no exista score de algunos usuarios, en estos casos simplemente los consideramos neutros (score=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    (1, 'pablo honey'),\n",
    "    (2, 'the bends'),\n",
    "    (3, 'ok computer'),\n",
    "    (4, 'kid a'),\n",
    "    (5, 'amnesiac'),\n",
    "    (6, 'hail to the thief'),\n",
    "    (7, 'in rainbows'),\n",
    "    (8, 'the king of limbs'),\n",
    "    (9, 'a moon shaped pool')\n",
    "]\n",
    "\n",
    "scores = [\n",
    "    ('thom', 'pablo', 1),\n",
    "    ('thom', 'honey', 1),\n",
    "    ('martin', 'pablo', -1),\n",
    "    ('martin', 'honey', -1),\n",
    "    ('martin', 'ok', 30),\n",
    "    ('martin', 'computer', 30)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documentsRDD = sc.parallelize(documents)\n",
    "scoresRDD = sc.parallelize(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pablo', 1),\n",
       " ('honey', 1),\n",
       " ('the', 2),\n",
       " ('bends', 2),\n",
       " ('ok', 3),\n",
       " ('computer', 3),\n",
       " ('kid', 4),\n",
       " ('a', 4),\n",
       " ('amnesiac', 5),\n",
       " ('hail', 6),\n",
       " ('to', 6),\n",
       " ('the', 6),\n",
       " ('thief', 6),\n",
       " ('in', 7),\n",
       " ('rainbows', 7),\n",
       " ('the', 8),\n",
       " ('king', 8),\n",
       " ('of', 8),\n",
       " ('limbs', 8),\n",
       " ('a', 9),\n",
       " ('moon', 9),\n",
       " ('shaped', 9),\n",
       " ('pool', 9)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generamos registros del tipo (word, docId) donde word sera nuestra key\n",
    "words = documentsRDD.flatMap(lambda a: [(word, a[0]) for word in a[1].split()])\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pablo', ('thom', 1)),\n",
       " ('honey', ('thom', 1)),\n",
       " ('pablo', ('martin', -1)),\n",
       " ('honey', ('martin', -1)),\n",
       " ('ok', ('martin', 30)),\n",
       " ('computer', ('martin', 30))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llevamos scores a una representacion donde word sea la clave\n",
    "scoresRDDForJoin = scoresRDD.map(lambda a: (a[1],(a[0],a[2])))\n",
    "scoresRDDForJoin.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('honey', (1, ('thom', 1))),\n",
       " ('honey', (1, ('martin', -1))),\n",
       " ('ok', (3, ('martin', 30))),\n",
       " ('pablo', (1, ('thom', 1))),\n",
       " ('pablo', (1, ('martin', -1))),\n",
       " ('computer', (3, ('martin', 30)))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_join_scores = words.join(scoresRDDForJoin)\n",
    "words_join_scores.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 'thom'), 1),\n",
       " ((1, 'martin'), -1),\n",
       " ((3, 'martin'), 30),\n",
       " ((1, 'thom'), 1),\n",
       " ((1, 'martin'), -1),\n",
       " ((3, 'martin'), 30)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# para poder realizar una acumulacion debemos llevar la informacion a la representacion ((docId, userId), score) \n",
    "words_join_scores_mapped = words_join_scores.map(lambda a: ((a[1][0], a[1][1][0]), a[1][1][1]))\n",
    "words_join_scores_mapped.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 'martin', 60), (1, 'martin', -2), (1, 'thom', 2)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# acumulamos y llevamos a la representación pedida\n",
    "words_join_scores_mapped.reduceByKey(lambda a,b: a + b)\\\n",
    "                        .map(lambda a: (a[0][0], a[0][1], a[1]))\\\n",
    "                        .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2017-01 Parcial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tiene información estadística de la temporada regular de todos los jugadores de la NBA en un RDD de tuplas con el siguiente formato: (id_jugador, nombre, promedio_puntos, promedio_asistencias, promedio_robos, promedio_bloqueos, promedio_rebotes, promedio_faltas).\n",
    "\n",
    "Un analista de la cadena ESPN está trabajando con un RDD que corresponde a la primera ronda de playoffs y que tiene el siguiente formato: (id_jugador, id_partido, timestamp, cantidad_puntos, cantidad_rebotes, cantidad_bloqueos, cantidad_robos, cantidad_asistencias, cantidad_faltas).\n",
    "\n",
    "En base a estos RDDs se quiere programar en PySpark un programa que genere un RDD con los nombres (sin duplicados) de los jugadores que lograron en algún partido de playoffs una cantidad de asistencias mayor a su promedio histórico.\n",
    "\n",
    "##### Criterio\n",
    "\n",
    "Existen diversas maneras de realizarlo, pero es posible resolverlo realizando un reduceByKey con clave (id_jugador,id_partido) acumulando la cantidad de asistencias por partido quedando ((id_jugador, id_partido), sum_asistencias).\n",
    "\n",
    "Luego efectuamos un cambio de clave a id_jugador con un map para poder realizar un join de la informacion de la temporada usando id_jugador.\n",
    "\n",
    "Una vez hecho esto, filtramos aquella informacion que cumple de mayor cantidad asistencias en el partido que el promedio de la temporada.\n",
    "\n",
    "Dado que pueden quedarnos duplicados (un jugador podria superar su promedio de temporada en varios partidos), es importante al mapearla informacion al formato final realizar un distinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos para simplificar el formato, que puede obtenerse con un map.\n",
    "# (id_jugador, nombre, promedio_asistencias)\n",
    "players_all_time_stats = [\n",
    "    (1, 'Manu Ginobili', 800),\n",
    "    (2, 'Kobe Bryant', 100),\n",
    "    (3, 'Marc Gasol', 25),\n",
    "    (4, 'James Harden', 1000)]\n",
    "\n",
    "# usamos para simplificar el formato, que puede obtenerse con un map.\n",
    "# (id_jugador, id_partido, timestamp, cantidad_asistencias)\n",
    "scores = [\n",
    "  (1, 1, 1, 100),\n",
    "  (1, 1, 3, 100),\n",
    "  (2, 1, 1, 150),\n",
    "  (2, 1, 3, 150),\n",
    "  (3, 2, 2, 50),\n",
    "  (3, 2, 3, 50),      \n",
    "  (1, 2, 1, 150),\n",
    "  (1, 2, 3, 150),\n",
    "]\n",
    "\n",
    "players_all_time_stats_rdd = sc.parallelize(players_all_time_stats)\n",
    "scores_rdd = sc.parallelize(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 1), 200), ((1, 2), 300), ((2, 1), 300), ((3, 2), 100)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cambio de clave a id_jugador, partido\n",
    "scores_by_match = scores_rdd.map(lambda a: ((a[0],a[1]),a[3]))\n",
    "scores_by_match = scores_by_match.reduceByKey(lambda a,b: a+b)\n",
    "scores_by_match.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 200), (1, 300), (2, 300), (3, 100)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cambio de clave a id_jugador\n",
    "scores_by_player = scores_by_match.map(lambda a: (a[0][0], a[1]))\n",
    "scores_by_player.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (1, 'Manu Ginobili', 800)),\n",
       " (2, (2, 'Kobe Bryant', 100)),\n",
       " (3, (3, 'Marc Gasol', 25)),\n",
       " (4, (4, 'James Harden', 1000))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# juntamos los datos para poder evaluar lo pedido\n",
    "# preparamos la key del lado de informacion historica de jugadores.\n",
    "\n",
    "all_time_with_key = players_all_time_stats_rdd.map(lambda a: (a[0],a))\n",
    "all_time_with_key.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (200, (1, 'Manu Ginobili', 800))),\n",
       " (1, (300, (1, 'Manu Ginobili', 800))),\n",
       " (2, (300, (2, 'Kobe Bryant', 100))),\n",
       " (3, (100, (3, 'Marc Gasol', 25)))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_by_player = scores_by_player.join(all_time_with_key)\n",
    "scores_by_player.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, (300, (2, 'Kobe Bryant', 100))), (3, (100, (3, 'Marc Gasol', 25)))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overperformers = scores_by_player.filter(lambda a: a[1][0] > a[1][1][2])\n",
    "overperformers.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Marc Gasol', 'Kobe Bryant']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# veamos los duplicados\n",
    "overperformers = overperformers.map(lambda a: a[1][1][1]).distinct()\n",
    "overperformers.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
